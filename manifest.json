{
  "projectName": "MedicalRAGChatbot",
  "language": "Python",
  "frameworks": ["FastAPI", "LlamaIndex", "LangChain"],
  "entryPoints": {
    "ingestion": "rag_chatbot/scripts/ingest.py",
    "webService": "rag_chatbot/app/main.py"
  },
  "files": [
    {
      "path": "rag_chatbot/scripts/ingest.py",
      "summary": "Data ingestion pipeline: parses PDFs, chunks content, stores in MongoDB and Pinecone.",
      "type": "script"
    },
    {
      "path": "rag_chatbot/app/main.py",
      "summary": "FastAPI web server exposing the /chat endpoint.",
      "type": "web_service"
    },
    {
      "path": "rag_chatbot/app/services/agent/simple_flow.py",
      "summary": "Core RAG workflow orchestrator.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/core/config.py",
      "summary": "Centralized application configuration from environment variables.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/core/embeddings.py",
      "summary": "Custom client for a remote embedding model API.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/core/llm.py",
      "summary": "Client for the 'Omega' large language model API.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/core/extractor.py",
      "summary": "Regex-based medical entity extractor.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/services/query_engine/search.py",
      "summary": "Provides vector search (Pinecone) and reranking (Cross-Encoder).",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/services/query_engine/context.py",
      "summary": "Assembles context from MongoDB with semantic deduplication.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/services/query_engine/generation.py",
      "summary": "Generates final answers using the LLM.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/services/query_engine/transformation.py",
      "summary": "Transforms queries (condensing, sub-query generation).",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/models/data_models.py",
      "summary": "Pydantic models for document chunks and metadata.",
      "type": "module"
    },
    {
      "path": "rag_chatbot/app/models/agent_models.py",
      "summary": "Pydantic models for the agent's internal state (sub-queries).",
      "type": "module"
    }
  ],
  "callGraphEdges": [
    {"source": "main.py", "target": "simple_flow.py"},
    {"source": "simple_flow.py", "target": "transformation.py"},
    {"source": "simple_flow.py", "target": "search.py"},
    {"source": "simple_flow.py", "target": "context.py"},
    {"source": "simple_flow.py", "target": "generation.py"},
    {"source": "search.py", "target": "embeddings.py"},
    {"source": "search.py", "target": "pinecone"},
    {"source": "context.py", "target": "mongodb"},
    {"source": "generation.py", "target": "llm.py"},
    {"source": "transformation.py", "target": "llm.py"},
    {"source": "ingest.py", "target": "llama_parse"},
    {"source": "ingest.py", "target": "embeddings.py"},
    {"source": "ingest.py", "target": "mongodb"},
    {"source": "ingest.py", "target": "pinecone"},
    {"source": "ingest.py", "target": "extractor.py"}
  ],
  "neo4jMappings": [
    {
      "sourceFile": "ingest.py",
      "operation": "WRITE",
      "nodeLabel": "Chunk",
      "properties": ["chunk_id", "parent_id", "doc_id", "pdf_name", "page_no", "chunk_type", "text"],
      "description": "Stores parent and child chunks in MongoDB, which can be modeled as (:Chunk) nodes."
    },
    {
      "sourceFile": "ingest.py",
      "operation": "WRITE",
      "relationshipType": "HAS_EMBEDDING",
      "description": "Upserts vector embeddings for each chunk into Pinecone. This can be modeled as (:Chunk)-[:HAS_EMBEDDING]->(:Vector)."
    },
    {
      "sourceFile": "context.py",
      "operation": "READ",
      "nodeLabel": "Chunk",
      "queryPattern": "MATCH (c:Chunk) WHERE c.chunk_id IN [...]",
      "description": "Reads parent chunks from MongoDB based on IDs retrieved from the search phase."
    }
  ],
  "suggestedUnitTests": [
    {"file": "search.py", "testName": "test_reranker_boosts_relevant_doc", "description": "Assert that the medical reranker correctly boosts the score of a document containing relevant medical terms from the query."},
    {"file": "context.py", "testName": "test_semantic_deduplication", "description": "Assert that given two semantically similar documents, the assembler correctly identifies and removes one."},
    {"file": "transformation.py", "testName": "test_subquery_generator_regex_fallback", "description": "Assert that the sub-query generator can gracefully fall back to regex parsing if the LLM fails to return valid JSON."},
    {"file": "main.py", "testName": "test_chat_history_is_isolated", "description": "Assert that chat history is not shared between different users/sessions (requires fixing the global variable first)."},
    {"file": "ingest.py", "testName": "test_phi_detection", "description": "Assert that the `_detect_phi` function correctly identifies text containing personally identifiable health information."}
  ],
  "highRiskIssues": [
    {"rank": 1, "file": "main.py", "issue": "Shared Chat History", "description": "The `chat_history_store` is a global variable, causing all users to share the same conversation history. This is a critical privacy and security flaw."},
    {"rank": 2, "file": "ingest.py", "issue": "PHI Handling", "description": "The system detects but does not redact or anonymize Personally Identifiable Health Information (PHI), storing it in the database."},
    {"rank": 3, "file": "simple_flow.py", "issue": "Prompt Injection Vulnerability", "description": "User input is directly concatenated into LLM prompts, creating a risk of prompt injection attacks."},
    {"rank": 4, "file": "llm.py", "issue": "Long Timeout", "description": "A 10-minute timeout on LLM calls could lead to thread exhaustion and denial of service if the LLM API is slow or unresponsive."},
    {"rank": 5, "file": "context.py", "issue": "Potential for Inefficient Deduplication", "description": "The O(N^2) semantic deduplication algorithm could become a performance bottleneck if the number of retrieved documents is large."}
  ],
  "recommendedImprovements": [
    {"rank": 1, "file": "main.py", "improvement": "Implement Session-Based Chat History", "description": "Replace the global chat history with a session-managed store (e.g., using FastAPI dependencies and cookies/tokens) to ensure user isolation."},
    {"rank": 2, "file": "generation.py", "improvement": "Programmatically Append Disclaimer", "description": "Append the legal disclaimer to the LLM response in the code instead of relying on a prompt instruction to ensure it is always present."},
    {"rank": 3, "file": "search.py", "improvement": "Centralize Configuration", "description": "Move hardcoded values like the reranker weights and the Pinecone index name to the central config.py file."},
    {"rank": 4, "file": "ingest.py", "improvement": "Implement Incremental Ingestion", "description": "Modify the ingestion script to check for new or updated files instead of clearing and re-ingesting the entire dataset each time."},
    {"rank": 5, "file": "All Files", "improvement": "Add Unit and Integration Tests", "description": "Create a test suite to validate the functionality of individual components and the end-to-end flow, improving reliability and maintainability."}
  ]
}
